{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3.1 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 10.2.1 (GCC) 20200726\n",
      "  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libsrt --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libgsm --disable-w32threads --enable-libmfx --enable-ffnvcodec --enable-cuda-llvm --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt --enable-amf\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "[dshow @ 000001c64ef3e940] DirectShow video devices (some may be both video and audio devices)\n",
      "[dshow @ 000001c64ef3e940]  \"OBS Virtual Camera\"\n",
      "[dshow @ 000001c64ef3e940]     Alternative name \"@device_sw_{860BB310-5D01-11D0-BD3B-00A0C911CE86}\\{A3FCE0F5-3493-419F-958A-ABA1250EC20B}\"\n",
      "[dshow @ 000001c64ef3e940] DirectShow audio devices\n",
      "[dshow @ 000001c64ef3e940]  \"Microphone (Blue Snowball)\"\n",
      "[dshow @ 000001c64ef3e940]     Alternative name \"@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{E97F871E-F5BA-4666-B9F2-B3327CC3322E}\"\n",
      "[dshow @ 000001c64ef3e940]  \"Microphone (Steam Streaming Microphone)\"\n",
      "[dshow @ 000001c64ef3e940]     Alternative name \"@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{D8053AC5-859D-4D28-8EA6-ADE4F0679490}\"\n",
      "dummy: Immediate exit requested\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -f dshow -list_devices true -i dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# The data acquisition process will stop after this number of steps.\n",
    "# This eliminates the need of process synchronization and makes this\n",
    "# tutorial simple.\n",
    "NUM_ITER = 100\n",
    "\n",
    "\n",
    "def stream(q, format, src, segment_length, sample_rate):\n",
    "    from torchaudio.io import StreamReader\n",
    "\n",
    "    print(\"Building StreamReader...\")\n",
    "    streamer = StreamReader(src, format=format)\n",
    "    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate)\n",
    "\n",
    "    print(streamer.get_src_stream_info(0))\n",
    "    print(streamer.get_out_stream_info(0))\n",
    "\n",
    "    print(\"Streaming...\")\n",
    "    print()\n",
    "    stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\n",
    "    for _ in range(NUM_ITER):\n",
    "        (chunk,) = next(stream_iterator)\n",
    "        q.put(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Build inference pipeline from RNNTBundle.\n",
    "\n",
    "    Args:\n",
    "        bundle (torchaudio.pipelines.RNNTBundle): Bundle object\n",
    "        beam_width (int): Beam size of beam search decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):\n",
    "        self.bundle = bundle\n",
    "        self.feature_extractor = bundle.get_streaming_feature_extractor()\n",
    "        self.decoder = bundle.get_decoder()\n",
    "        self.token_processor = bundle.get_token_processor()\n",
    "\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        self.state = None\n",
    "        self.hypothesis = None\n",
    "\n",
    "    def infer(self, segment: torch.Tensor) -> str:\n",
    "        \"\"\"Perform streaming inference\"\"\"\n",
    "        features, length = self.feature_extractor(segment)\n",
    "        hypos, self.state = self.decoder.infer(\n",
    "            features, length, self.beam_width, state=self.state, hypothesis=self.hypothesis\n",
    "        )\n",
    "        self.hypothesis = hypos[0]\n",
    "        transcript = self.token_processor(self.hypothesis[0], lstrip=False)\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextCacher:\n",
    "    \"\"\"Cache the end of input data and prepend the next input data with it.\n",
    "\n",
    "    Args:\n",
    "        segment_length (int): The size of main segment.\n",
    "            If the incoming segment is shorter, then the segment is padded.\n",
    "        context_length (int): The size of the context, cached and appended.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment_length: int, context_length: int):\n",
    "        self.segment_length = segment_length\n",
    "        self.context_length = context_length\n",
    "        self.context = torch.zeros([context_length])\n",
    "\n",
    "    def __call__(self, chunk: torch.Tensor):\n",
    "        if chunk.size(0) < self.segment_length:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n",
    "        chunk_with_context = torch.cat((self.context, chunk))\n",
    "        self.context = chunk[-self.context_length :]\n",
    "        return chunk_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(device, src, bundle):\n",
    "    print(torch.__version__)\n",
    "    print(torchaudio.__version__)\n",
    "\n",
    "    print(\"Building pipeline...\")\n",
    "    pipeline = Pipeline(bundle)\n",
    "\n",
    "    sample_rate = bundle.sample_rate\n",
    "    segment_length = bundle.segment_length * bundle.hop_length\n",
    "    context_length = bundle.right_context_length * bundle.hop_length\n",
    "\n",
    "    print(f\"Sample rate: {sample_rate}\")\n",
    "    print(f\"Main segment: {segment_length} frames ({segment_length / sample_rate} seconds)\")\n",
    "    print(f\"Right context: {context_length} frames ({context_length / sample_rate} seconds)\")\n",
    "\n",
    "    cacher = ContextCacher(segment_length, context_length)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def infer():\n",
    "        for _ in range(NUM_ITER):\n",
    "            chunk = q.get()\n",
    "            segment = cacher(chunk[:, 0])\n",
    "            transcript = pipeline.infer(segment)\n",
    "            print(transcript, end=\"\", flush=True)\n",
    "\n",
    "    import torch.multiprocessing as mp\n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    q = ctx.Queue()\n",
    "    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n",
    "    p.start()\n",
    "    infer()\n",
    "    p.join()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "0.13.1+cu116\n",
      "Building pipeline...\n",
      "Sample rate: 16000\n",
      "Main segment: 2560 frames (0.16 seconds)\n",
      "Right context: 640 frames (0.04 seconds)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        device=\"dshow\",\n",
    "        src=\"audio=@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{E97F871E-F5BA-4666-B9F2-B3327CC3322E}\",\n",
    "        bundle=torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml-ulLy1S9H-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:30:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99dfab03f6bfe1b4cdeae3cc60fa8b722546d96b54be4105f2ad3346a439e1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
